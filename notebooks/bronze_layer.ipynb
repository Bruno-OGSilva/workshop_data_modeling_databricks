{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "228d9a1a-4c61-4300-8838-206a79854ebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b97bb6cb-7ec6-4e13-8601-fc508259deb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 0) Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fad7a666-94cc-43b5-8d48-73ef22e88328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks: instalar Faker (persistente no cluster enquanto ativo)\n",
    "# Se seu cluster já tem Faker, pode ignorar esta célula.\n",
    "%pip install Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "082b2ba9-f305-46be-8b84-aa0c0be159de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1) Parâmetros e helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fdb577f-738a-4af2-9c57-1d975461e90e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "import random\n",
    "import string\n",
    "\n",
    "# ===== Parâmetros =====\n",
    "CATALOG = \"workshop_modelagem_aovivo\"            # exemplo: \"workshop_catalog\" ou None se não usar Unity Catalog\n",
    "SCHEMA  = \"bronze\"        # esquema/database onde serão criadas as tabelas\n",
    "SEED    = 42\n",
    "\n",
    "N_CUSTOMERS   = 1000\n",
    "N_PRODUCTS    = 500\n",
    "N_ORDERS      = 5000\n",
    "N_ORDER_ITEMS = 12000\n",
    "\n",
    "# Percentuais de \"problemas\"\n",
    "P_DUP_ORDERS                = 0.025   # ~2.5% order_id duplicados\n",
    "P_NULL_CUSTOMER_IN_ORDERS   = 0.05    # ~5% customer_id nulos\n",
    "P_STATUS_CASE_VARIATION     = 0.25\n",
    "P_STRING_DATE_IN_ORDERS     = 0.50    # metade como string, metade como date coerente\n",
    "P_STRING_NUMERIC_IN_FIELDS  = 0.15    # % de numéricos como string\n",
    "\n",
    "P_DUP_PRODUCT_ID            = 0.03\n",
    "P_INCONSISTENT_IS_ACTIVE    = 0.40\n",
    "P_NULL_BRAND_SUBCATEGORY    = 0.10\n",
    "\n",
    "P_CUSTOMER_INCONSISTENCY    = 0.20    # estados \"SP\", \"sp\", \"São Paulo\"\n",
    "P_CUSTOMER_DUP_DIFF_UPDATE  = 0.10    # duplicar customer_id com last_update_date diferente\n",
    "P_EMPTY_FIELDS_CUSTOMER     = 0.05\n",
    "\n",
    "P_DUP_ORDERITEM_SAME_OP     = 0.05    # duplicar (order_id, product_id) com updated_at diferente\n",
    "P_NULLS_DISCOUNT_PROMO      = 0.15\n",
    "\n",
    "random.seed(SEED)\n",
    "fake = Faker(\"pt_BR\")\n",
    "Faker.seed(SEED)\n",
    "\n",
    "# ===== Nome totalmente qualificado de tabela =====\n",
    "def fqtn(table):\n",
    "    if CATALOG:\n",
    "        return f\"`{CATALOG}`.`{SCHEMA}`.`{table}`\"\n",
    "    else:\n",
    "        return f\"`{SCHEMA}`.`{table}`\"\n",
    "\n",
    "# ===== Criar schema/database =====\n",
    "if CATALOG:\n",
    "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{CATALOG}`\")\n",
    "    spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS `{CATALOG}`.`{SCHEMA}`\")\n",
    "else:\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS `{SCHEMA}`\")\n",
    "\n",
    "# ===== Utilidades =====\n",
    "STATUSES = [\"delivered\",\"shipped\",\"processing\",\"cancelled\",\"returned\"]\n",
    "\n",
    "def random_status_inconsistent():\n",
    "    s = random.choice(STATUSES)\n",
    "    if random.random() < P_STATUS_CASE_VARIATION:\n",
    "        # variações de capitalização\n",
    "        choices = [s.upper(), s.capitalize(), s.lower()]\n",
    "        s = random.choice(choices)\n",
    "    return s\n",
    "\n",
    "def random_date_between(days_back=365):\n",
    "    base = datetime.utcnow()\n",
    "    delta = timedelta(days=random.randint(0, days_back), seconds=random.randint(0, 86399))\n",
    "    d = base - delta\n",
    "    return d\n",
    "\n",
    "def random_date_mixed_formats(dt):\n",
    "    # retorna string em formatos variados (\"/\", \"-\", com/sem tempo)\n",
    "    # ex.: \"2025-10-25\", \"2025/10/25 14:33:20\", \"25/10/2025\", etc.\n",
    "    formats = [\n",
    "        \"%Y-%m-%d\",\n",
    "        \"%Y/%m/%d\",\n",
    "        \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%d/%m/%Y\",\n",
    "        \"%d-%m-%Y %H:%M:%S\",\n",
    "    ]\n",
    "    return dt.strftime(random.choice(formats))\n",
    "\n",
    "def maybe_stringify_number(x):\n",
    "    # converte numérico para string em parte dos casos\n",
    "    if random.random() < P_STRING_NUMERIC_IN_FIELDS:\n",
    "        return f\"{x}\"\n",
    "    return x\n",
    "\n",
    "def maybe_null(val, p=0.1):\n",
    "    return None if random.random() < p else val\n",
    "\n",
    "def dirty_state(uf):\n",
    "    # introduz inconsistências: \"SP\", \"sp\", \"São Paulo\"\n",
    "    if random.random() < P_CUSTOMER_INCONSISTENCY:\n",
    "        variants = [uf, uf.lower(), \"São Paulo\" if uf.upper()==\"SP\" else uf]\n",
    "        return random.choice(variants)\n",
    "    return uf\n",
    "\n",
    "def random_bool_inconsistent():\n",
    "    # \"true\", \"1\", \"yes\", True, False...\n",
    "    opts = [\"true\",\"1\",\"yes\",\"false\",\"0\",\"no\", True, False]\n",
    "    if random.random() < P_INCONSISTENT_IS_ACTIVE:\n",
    "        return random.choice(opts)\n",
    "    return True\n",
    "\n",
    "def alnum(n=8):\n",
    "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=n))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
